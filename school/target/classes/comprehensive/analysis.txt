CS 2420 Assignment 11: Random Phrase Generator - Analysis
=========================================================

1. PARTNER INFORMATION AND SUBMISSION
-------------------------------------
This is a solo submission. I have completed all work independently.


2. SOLUTION DESCRIPTION
-----------------------
Our solution generates random phrases from context-free grammars using two strategies:

For small grammars (≤100,000 unique phrases):
- Enumerate every possible phrase upfront
- Store in array, then randomly pick from it
- Generating N phrases = O(N) after one-time enumeration

For large grammars (>100,000 unique phrases):
- Expand grammar using a stack, making random choices at each non-terminal
- Each phrase is independently generated
- No memory explosion from storing billions of possibilities

The crossover point (100,000) was determined experimentally - below this, 
precomputation wins; above this, the memory cost isn't worth it.


3. PARSING, GRAMMAR REPRESENTATION, AND RANDOM GENERATION
----------------------------------------------------------

PARSING:
The grammar file format has sections like:
  {
  <non-terminal>
  production1
  production2 | production3
  }

I read each section, extract the non-terminal name, then parse each production.
Symbols in angle brackets (<word>) are non-terminals; everything else is a terminal.

GRAMMAR REPRESENTATION:
I use a 3D int array: grammar[nonTerminalId][productionIndex][symbolIndex]

Here's why each dimension exists. Consider this simple grammar:
  <sentence> → <subject> <verb>
  <subject> → the dog | a cat  
  <verb> → runs | jumps | sleeps

If <sentence>=0, <subject>=1, <verb>=2, and terminals are encoded as negative 
numbers (the=-1, dog=-2, a=-3, cat=-4, runs=-5, jumps=-6, sleeps=-7):

  grammar[0] = {{1, 2}}           // <sentence> has 1 production: [<subject>, <verb>]
  grammar[1] = {{-1, -2}, {-3, -4}}  // <subject>: "the dog" or "a cat"
  grammar[2] = {{-5}, {-6}, {-7}}    // <verb>: "runs", "jumps", or "sleeps"

Why ints instead of strings?
- "the" as a string = 24 bytes (object header + char array + length)
- "the" as int -1 = 4 bytes
- We do millions of symbol comparisons; checking "symbol < 0" is one CPU instruction,
  while "symbol instanceof Terminal" involves virtual method dispatch

RANDOM GENERATION (Stack-based):
1. Push start symbol onto stack
2. While stack not empty:
   - Pop symbol
   - If terminal: append to output
   - If non-terminal: pick random production, push its symbols (right to left)
3. Return accumulated string


4. DATA STRUCTURES USED AND CONSIDERED
--------------------------------------

USED:

int[][][] grammar - Primary grammar storage
  Why: Primitive arrays have zero object overhead per element. A 3D structure 
  naturally maps to "which non-terminal → which production → which symbol."
  The int encoding lets us check terminal vs non-terminal with one comparison.

int[] expansionStack - For stack-based generation
  Why: Faster than ArrayDeque for primitive ints (no boxing/unboxing).

String[] allPhrases - For precomputed strategy
  Why: After enumeration, we just need random access. Arrays are fastest for 
  indexed access. Power-of-2 sizing lets us use bitwise AND instead of modulo.

StringBuilder with 1MB buffer - For output accumulation
  Why: System.out.println() per phrase is extremely slow due to I/O overhead.
  Batching output reduces system calls from N to N/batch_size.

XorShift64 random number generator
  Why: Benchmarked against java.util.Random and ThreadLocalRandom. XorShift64 
  was ~2x faster for our use case (just need decent randomness, not crypto-grade).

CONSIDERED BUT REJECTED:

HashMap<String, List<Production>> - Standard grammar representation
  Why rejected: String hashing is expensive, and we access grammar millions of 
  times. Integer indexing is O(1) with no hash computation.

ArrayList for productions
  Why rejected: ArrayList has object overhead and bounds checking. We know 
  production sizes at parse time, so fixed arrays are better.

Recursive expansion (method calls instead of explicit stack)
  Why rejected: Method call overhead adds up. An explicit int[] stack avoids 
  creating stack frames and allows loop optimizations.


5. TIMING EXPERIMENTS
---------------------

I designed three experiments to measure different aspects of performance:

EXPERIMENT 1: Time vs Number of Phrases (Precomputed Strategy)
[experiment1_phrases.png]

Tests how generation time scales with output size when using precomputation.
After the one-time enumeration cost, each additional phrase is just an array 
lookup and string copy.

Data collected for 10,000 to 250,000 phrases on a grammar with 1,620 unique phrases.
Results: Linear scaling as expected - twice as many phrases takes twice as long.
The time is dominated by the random selection and output construction, not 
grammar processing.

EXPERIMENT 2: Time vs Non-Terminal Count (Stack-Based Strategy)
[experiment2_nonterminals.png]

Tests how grammar complexity affects stack-based generation.
Each non-terminal requires a stack push, random choice, and expansion.

Data collected for 500 to 6,000 non-terminals, generating 10,000 phrases each.
Results: Linear scaling - more non-terminals means more work per phrase.
The stack-based approach processes each symbol once, so complexity is 
proportional to the number of symbols encountered.

EXPERIMENT 3: Precomputed vs Stack-Based Comparison
[experiment3_comparison.png]

This experiment answers: "At what point does each strategy win?"

Data collected from 25,000 to 500,000 unique phrases (phrase_count parameter).
I generated 10,000 phrases from each grammar.

Results:
- Below ~100,000 phrases: Precomputed is faster (43-45ms range)
- Above ~300,000 phrases: Stack-based wins (45-50ms vs 60-77ms)
- Crossover region: 100,000-300,000 phrases

The precomputed line shows increasing time because:
- Larger grammars = more enumeration work
- Larger arrays = more memory pressure

The stack-based line shows variance because:
- Random phrase lengths (sometimes "word", sometimes "word word word word...")
- Each run does different amounts of work

This experiment justified the 100,000 threshold - it's where precomputation 
stops being worth the enumeration cost.

EXPECTED RUNNING-TIME BEHAVIOR:

Experiment 1 (Precomputed, varying phrase count):
- Expected: O(N) where N = number of phrases to generate
- Observed: Linear growth confirmed
- Reason: Each phrase is a constant-time array access + string copy

Experiment 2 (Stack-based, varying complexity):
- Expected: O(K) where K = number of non-terminals
- Observed: Linear growth confirmed
- Reason: Work per phrase scales with grammar size

Experiment 3 (Strategy comparison):
- Expected: Precomputed grows with grammar size, stack-based stays flat
- Observed: Exactly this pattern, with crossover around 300,000 phrases
- Reason: Enumeration is O(P) where P = total phrases; stack-based is O(depth)


6. EFFICIENCY
-------------

ASYMPTOTIC EFFICIENCY:

Parsing: O(G) where G = grammar file size
- Each character is read once, each production is processed once

Precomputed generation: O(E + N)
- E = enumeration cost (one-time, proportional to total phrases)
- N = number of phrases to output

Stack-based generation: O(N × D)
- N = number of phrases
- D = average phrase depth (symbols expanded per phrase)

Space: O(P × L) for precomputed, O(D) for stack-based
- P = number of precomputed phrases, L = average phrase length
- D = maximum recursion depth (typically small, ~10-20)

PROGRAMMER EFFICIENCY:

The hardest part was getting the encoding right. Representing symbols as ints 
saves memory and speeds up comparisons, but you have to be careful:
- Non-terminals: 0, 1, 2, ... (index into grammar array)
- Terminals: -1, -2, -3, ... (use ~index to get string from terminals array)

The trickiest bug was pushing symbols in wrong order. Stack is LIFO, so 
"<subject> <verb>" must push <verb> first, then <subject>.

What I'd do differently next time:
- Write the simple HashMap version first, verify correctness, then optimize
- Add more assertion checks during development
- Time individual components earlier to find bottlenecks

Code organization: ~640 lines total
- Kept everything in one class for cache locality
- Javadoc on public methods, inline comments on tricky sections
- Named constants for magic numbers (THRESHOLD, BUFFER_SIZE, etc.)

The optimization that helped most: XorShift64 + loop unrolling. 
The 8x unrolled loop generates 8 phrases per iteration, reducing loop overhead.
Combined with fast random numbers, this gave the biggest speedup.
